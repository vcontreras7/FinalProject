---
title: "Final Project"
author: "Andrew Contreras"
date: "12/10/2019"
output: 
  html_document:
  toc: true
  levels: 2
  number_sections: true
---

```{r setup }
knitr::opts_chunk$set(echo = TRUE)
library(DMwR2)
library(cluster)
library(factoextra)
library(tidyverse)
```

**ASSIGNMENT:**    

1. Summary of Methodology

First include the data set involving sales transactions from store. This data is split in categories from the transactions: Quatnitites/Price Points. Using a scale function of 1 to -1 in to scale the values which in terms is normalizing the data. In order to analyze and compare the variance between variables we will need to control these different units. The next step is to form clusters which the first priority is that 100 percent of full fraud data is conirmed. The optimal data center was at the 6 mark "the knee". Once this was completed we reffered to the unknown machine data. Then unsupervised transactions highlight that anything surrounding the cluster have similiar traits fraud. By means of similiarity does not neccesarily mean 100% certainity. 



2. Identifiable patterns

The first identifiable patters of 50 and 100 were similiar to 100% fraud. This was in connection of 14 ID's. In order to create a large sample size either by double or tripling nstart was the variable that needed to be change. By doubling the sample size the ID's grew 22 values. There is room for fraud based of large transaction in forms of volume or monetary value. 



# Unsupervised learning: cluster analysis
problem statiement: given a database of just over 400,000 transactions, each labeled as 'ok', 'fraud', or 'unkn' use a machine learning technique to identify 50 or so transactions that *may* by fraudulent

```{r dataSets, echo=FALSE, include=FALSE}
data() # to see all the data sets available
```

## load the sales data set  
notice that a variable was added: price per item

```{r}
data(sales)
salesDF = na.omit(sales)
salesDF$perItem = salesDF$Val/salesDF$Quant
salesDF$Quant = as.numeric(salesDF$Quant)
salesDF$Val = as.numeric(salesDF$Val)
head(salesDF)
```


This package includes functions and data accompanying the book
*Data Mining with R, learning with case studies* by Luis Torgo, CRC Press 2010

The sales data set consists of sale transaction reports.  there are just over 400,000 transactions.  The entries are 

Variable |  Description
---------|----------------
ID      | employee ID number   
Prod     | product ID number 
Quant    | quantity sold
Val     |  reported value of the transaction
Insp    | categorical variable: 'ok', 'fraud', 'unkn'

Only a small percentage of the transactions have been verified as either 'ok' or 'fraud.' 
The purpose of the study is to Use cluser analysis to identify the top 50 'unkn' transactions to be investigated for possible fraud

## Construct data set with 'known' transactions
This all transactions that are known to be either 'ok' or 'fraud,' look at the data

```{r}
fraudData = filter(salesDF, Insp=='fraud')
okData = filter(salesDF, Insp == 'ok')
fraud_ok = rbind(fraudData, okData)
```

### boxplots
notice that the range of data values is so large that log-transform was used

```{r}
nFraud = length(fraudData$Val)
print(nFraud)
nOK = length(okData$Val)
print(nOK)
names=c('fraud','ok')
boxplot(log10(fraudData$Val),log10(okData$Val), names=names, horizontal = TRUE, 
        main='Transaction Values\ntotal value', xlab = 'log $$')
grid()
summary(log10(fraudData$Val))
```


The above boxplot shows that both the fraud and ok transactions have about the same dollar distributions


By the the 'per-item' quantity

```{r}
boxplot(log10(fraudData$perItem), log10(okData$perItem), names=names, horizontal = TRUE, main='Transaction Values\nitem price', xlab = 'log price per item')
grid()
```

as may be expected the variability on fraudlent transactions is greater than that of the legit transactions

```{r}
boxplot(log10(fraudData$Quant), log10(okData$Quant), names=names, horizontal = TRUE, main='Quantity of transactions', xlab = 'log number of transactions')
grid()
```

boxplot shows little difference between the 'ok' and 'fraud' transactions.  Larger orders could represent more of the fraudulent kind?

## set up data set for clustering; 
This assumes that the variables are numeric, not categorical

```{r}
fraud_okLog = fraud_ok
fraud_okLog$Quant = log10(fraud_ok$Quant)
fraud_okLog$Val = log10(fraud_ok$Val)
fraud_okLog$perItem = log10(fraud_ok$perItem)
dfLog = fraud_okLog[,c(3,4,6)]
dfScaled = scale(fraud_ok[,c(3,4,6)])
```


##  Try a few clusters  
Multiple clusters...

```{r}
k2= kmeans(dfScaled, centers=3, nstart=25)
k3= kmeans(dfScaled, centers=4, nstart=25)
k4= kmeans(dfScaled, centers=5, nstart=25)
k5= kmeans(dfScaled, centers=6, nstart=25)

```

plots to compare

```{r comparePlots}
p1 = fviz_cluster(k2, geom='point', data =dfScaled) + ggtitle('k=3')
p2 = fviz_cluster(k3, geom='point', data =dfScaled) + ggtitle('k=4')
p3 = fviz_cluster(k4, geom='point', data =dfScaled) + ggtitle('k=5')
p4 = fviz_cluster(k5, geom='point', data =dfScaled) + ggtitle('k=6')
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow=2)
```

## How many clusters?   
look at the within cluster variability.   

```{r withinClusterPlot }
was = function(k) {
  kmeans(dfScaled, centers=k, nstart=25)$tot.withinss 
}
# compute and plot for k=1 to k=10
k.values=2:10
# extract was for 2-10 clusters
was.values = map_dbl(k.values, was)
print('silhouette values: ')
for (i in 2:10) print(paste(i," ", was.values[i]," ", was.values[i]/was.values[1]*100,"%"))
plot(k.values, was.values, type='b', pch=19, frame=FALSE, 
     xlab = 'number of clusters', ylab='total within-clusters SS')
grid()
```

From this plot it appears that 6 clusters is appropriate.. 

## final result: clusters
larger 'nstart' means better chance of finding a _global_ solution

```{r}
final = kmeans(dfScaled,6, nstart=100)
fviz_cluster(final, geom='point', data = dfScaled) + ggtitle('optimum clusters')
```

### print cluster results:

```{r echo = FALSE}
clusterCenterSave = matrix(data=0, nrow=2, ncol=3)
k=1
nObs = length(dfScaled[,1])
for (i in unique(final$cluster)){
  print(paste('Cluster ',i))
  clusterIndex = which(final$cluster==i)
  fraudCount = length(which(fraud_ok[clusterIndex,5] == 'fraud'))
  okCount = length(which(fraud_ok[clusterIndex,5]=='ok'))
  print(paste('fraud = ', fraudCount, ' ok = ', okCount))
  print(' Cluster mean point')
  print(paste('fraud = ', round(fraudCount/(fraudCount+okCount)*100),'%'))
  
  if(okCount ==0) {
    print(round(final$centers[i,],2))
    clusterCenterSave[k,]=final$centers[i,]
    k=k+1
  }
  
  print('-------')
}
```

User centers of the 'all fraud' clusters to find transactions near these 

```{r echo=FALSE}
sortIndexQ = sort(fraud_okLog$Quant, index.return=TRUE, decreasing = TRUE)
print('highest quantity sales')
print('Person ID      quantity     type')
for (i in 1:20) print(paste(fraud_okLog$ID[sortIndexQ$ix[i]], "      ",round(10^fraud_okLog$Quant[sortIndexQ$ix[i]]),"   ", fraud_okLog$Insp[sortIndexQ$ix[i]]))
#print(fraud_ok$ID[sortIndex$ix][1:20])
sortIndexV = sort(fraud_okLog$perItem, index.return=TRUE, decreasing = TRUE)
print('------------------------')
print('highest price items')
print('Person ID    item price    transaction')
for (i in 1:20) print(paste(fraud_okLog$ID[sortIndexV$ix][i],"   ",round(10^(fraud_okLog$perItem[sortIndexV$ix[i]]),4), "       ", fraud_okLog$Insp[sortIndexV$ix[i]]))
```

## Find possible fraudlent transactions
find transactions in the data frame Insp == 'unk' that are closest to the 'fraud' clusters

```{r}
print(clusterCenterSave)
unkDataFrame = filter(salesDF, Insp == 'unkn')
nUnkn = 0
nUnkn= length(unkDataFrame$ID)
```

find distances to 'fraud' clusters

```{r}
saveTrans = matrix(data=0, nrow=nUnkn, ncol=2)
scaledUnknown = data.frame(scale(unkDataFrame[,c(3,4,6)]))
for(i in 1:nUnkn) {
  
  delta1 = c(as.numeric(scaledUnknown[i,])-as.numeric(clusterCenterSave[1,]))
  delta1 = sqrt(sum(delta1^2))
  
  delta2 = c(as.numeric(scaledUnknown[i,])-as.numeric(clusterCenterSave[2,]))
  delta2 = sqrt(sum(delta2^2))
  
saveTrans[i,]=c(i, min(delta1, delta2))
}
```

find the 50 smallest distances

```{r}
sortSaveTrans = sort(saveTrans[,2], index.return=TRUE)
print(unkDataFrame[sortSaveTrans$ix[1:100],])
print(unique(unkDataFrame[sortSaveTrans$ix[1:100],]$ID))
```
```{r}
hist(saveTrans[sortSaveTrans$ix[1:100],2], main="dist to fraud", xlab='dist', freq=FALSE)
```


